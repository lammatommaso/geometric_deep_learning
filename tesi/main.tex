\documentclass[12pt,a4paper,twoside]{report}
\usepackage[english]{babel}
\usepackage{newlfont}
\usepackage{color}
\textwidth=450pt\oddsidemargin=0pt
\usepackage[utf8]{inputenc}
\usepackage{fouriernc}
\usepackage[T1]{fontenc}
\usepackage{adjustbox}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage[colorlinks=true,linkcolor=black,urlcolor=black, citecolor=black]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{subfiles}
\usepackage{comment}
\usepackage{float}
\usepackage{enumerate}
\usepackage{appendix}
\usepackage[style=numeric, backend=biber, sorting=nty]{biblatex}
\usepackage{chngcntr}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
% \fancyhead[LE]{\nouppercase \leftmark}
\fancyhead[R]{\nouppercase \rightmark}
\fancyfoot[R]{\thepage}

\fancypagestyle{plain}
{
    \fancyfoot[R]{\thepage}
    \fancyhead{}
    \renewcommand{\headrulewidth}{0pt}
}

% \renewcommand{\chaptermark}[1]{%
% \markboth{#1}{}}

\definecolor{SAEblue}{rgb}{0, .62, .91}
\renewcommand\theequation{{\color{SAEblue}\arabic{equation}}}

\addbibresource{bibliography.bib}

\setlength{\parindent}{0pt}
\makeatletter
\renewcommand{\fnum@figure}{Fig. \thefigure}
\makeatother


\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{exa}[thm]{Example}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{obs}[thm]{Observation}

% \AtAppendix{
%     \renewtheorem{thm}{Theorem}[chapter]
%     \renewtheorem{lem}[thm]{Lemma}
%     \renewtheorem{prop}[thm]{Proposition}
%     \renewtheorem{cor}[thm]{Corollary}
%     \renewtheorem{defn}[thm]{Definition}
%     \renewtheorem{exa}[thm]{Example}
%     \renewtheorem{rem}[thm]{Remark}
% }

%\setcounter{secnumdepth}{0} % no section numbering displayed, no idea how

\newcommand{\bb}[1]{\textbf{#1}}
\newcommand{\ii}[1]{\textit{#1}}
\newcommand{\eps}{\varepsilon}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\eval}[1]{\Big{|}_{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\bmc}[1]{\textbf\mathcal{#1}}
\newcommand{\scal}[2]{\langle #1, #2 \rangle}
\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\braket}[2]{\langle #1 | #2 \rangle}
\newcommand{\del}{\partial}
\newcommand{\then}{\Rightarrow}
\newcommand{\hull}[1]{\langle #1 \rangle}
\newcommand{\m}{\text{-}}
% \newcommand{\ch}[1]{\langle #1 \rangle}
\newcommand{\ch}[1]{\ket{#1}}
% \newcommand{\cc}[1]{\varphi_{\langle #1 \rangle}}
\newcommand{\cc}[1]{\bra{#1}}
% \newcommand{\scal}[2]{#1 \cdot #2 }
\newcommand{\im}{\mathrm{im}\,}
% \newcommand{\ker}{\mathrm{ker}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\dg}{deg}

\begin{document}
    \begin{titlepage}
        \subfile{sections/front}
    \end{titlepage}
    \begin{abstract}
        Lo scopo del geometric deep learning è quello di estendere l'algoritmo di deep learning sviluppato per la classificazione di immagini a domini
        non euclidei come grafi e complessi simpliciali.
        In questa tesi ci proponiamo di dare una definizione matematica dei concetti cardine utilizzati nel geometric deep learning quali equivarianza e convoluzione sui grafi.
        Vedremo inoltre come definire una rete convoluzionale invariante rispetto all'azione di gruppi.
    \end{abstract}
    \begin{abstract}
        The aim of geometric deep learning is to extend the deep learning algorithm developed for image classification to non euclidean domains such as graphs
        and simplicial complexes.
        In this thesis we try to give a mathematical definition of the main concepts used in geometric deep learning, e.g. graph convolution and equivariance.
        Furthermore we'll see how to define neural network which is invariant under the action of a group.
    \end{abstract}
    \pagenumbering{roman}
    %   \subfile{sections/abstract_ita}
    %    \newpage
    %    \subfile{sections/abstract_eng}
    %    \newpage
    \tableofcontents
    \newpage
    \pagenumbering{arabic}
    \begin{chapter}{Introduction}
        The last decade machine learning has witnessed an experimental revolution and
        many problems we thought to be computationally unsolvable are now feasible due to deep learning.
        Among these problems we have computer vision, playing Go and protein folding.
        Geometric deep learning appears to be a really efficient way to solve problems on non grid-like domains.
        The last formulation of geometric deep learning also provides us with necessary conditions for a convolutional
        network to be invariant w.r.t. some task-related symmetries.
        In this thesis we define the basic concepts needed for understanding the foundations of geometric deep learning,
        and we will analyze how symmetries can be used to reduce the number of parameters to be learnt.
        \medskip
        This work is organized as follows.

        \medskip
        In chapter 1 we recall some notions of topology, like
        simplicial complexes, homology and cohomology, which are needed
        for a sound treatment of the graph modeling of neural networks.
        Then in chapter 2 we introduce the notion of graph, together
        with graph homology and cohomology, necessary to properly
        introduce the Laplacian operator on graph. This operator
        is fundamental for a mathematically sound definition of convolutions on
        graph.
        Finally, in chapter 3 we describe geometric deep learning
        based on convolution on graph and the concepts of symmetry
        and invariance in this context.
    \end{chapter}
    \begin{chapter}{Preliminaries on topology}
        \label{ch:1}
        \subfile{sections/1}
    \end{chapter}
    \begin{chapter}{Graphs}
        \label{ch:2}
        \subfile{sections/2}
    \end{chapter}
    \begin{chapter}{Geometric Deep Learning}
        \label{ch:3}      
        \subfile{sections/3}
    \end{chapter}
    \newpage
%    \chapter*{Conclusion}
%         Most of the deep learning techniques used today are based on models which learn a partition of the set of smooth functions defined on euclidean 
%         domains into human friendly equivalence classes. Although this approach has been successful in modern machine learning, it only deals with a really 
%         small set of domains. The goal of geometric deep learning is to extend this method to data defined on manifolds and simplicial complexes.\\
%     \addcontentsline{toc}{chapter}{Conclusion}
    \begin{appendices}
    \counterwithin{thm}{chapter}
        \begin{chapter}{Category Theory}
            \label{app:A}
            \subfile{sections/appendixA/main}
        \end{chapter}
        \begin{chapter}{Dirac's Notation}
            \label{app:B}
            \subfile{sections/appendixB/main}
        \end{chapter}
        \begin{chapter}{Laplacian Operators}
            \label{app:C}
            \subfile{sections/appendixC/main}
        \end{chapter}
    \end{appendices}
    \newpage
    \printbibliography[heading = bibintoc]
    % \chapter*{Ringraziamenti}
    % \begin{flushright}
    %     \vspace{8cm}
    %     Vorrei ringraziare la mia famiglia per la possibilità,\\ i colleghi e gli amici per il supporto\\ e il relatore per la pazienza.
    % \end{flushright}
\end{document}
