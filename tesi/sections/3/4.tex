\documentclass[../3.tex]{subfiles}
\begin{document}
    In this section we will give an overview on modern geometric deep learning using the considerations of the previous section.
    Let $\mc{S}(\Omega,V)$,$\mc{S}(\Omega',V)$ and $\mc{S}(\Lambda,W)$.
    {\color{blue}
    As we did for graphs we can define the signal space as a free abelian group on $\Omega$ therefore every signal can be expressed as 
    \[ \ch{\psi} = \sum_{i \in \Omega} \psi_i\ch{i}, \]
    where the sum is the formal sum of the free abelian group.
    If we choose the generators $\Omega' = \{\ch{j}\}$ of a proper subgroup of this free abelian 
    group as the basis for our signals, we are restricting the signals into a lower dimensional signal space.
    In the following exposition we will consider $\mc{S}(\Omega',V)$ to be a lower dimensional version of $\mc{S}(\Omega,V)$.}
    From this setting we introduce the building blocks of geometric deep learning.

    \begin{itemize}
        \item A \ii{linear $G$-invariant layer} $B : \mc{S}(\Omega,V) \to \mc{S}(\Lambda,W)$ such that
            $B(g.\phi) = g.B(\phi)$ for all $g \in G$ and $\phi \in \mc{S}(\Omega,V)$.
        \item A \ii{nonlinearity} $\sigma : \mc{S}(\Omega,V) \to \mc{S}(\Lambda,W)$ which is applied element-wise 
            $(\sigma(\phi))(x) = \sigma'(\phi(x))$ where $\sigma' : V \to W$ is a non linear function.
        \item A \ii{$G$-equivariant local pooling} $P : \mc{S}(\Omega,V) \to \mc{S}(\Omega',V)$.
        \item A \ii{$G$-invariant global pooling} $P : \mc{S}(\Omega,V) \to \mc{C}$ such that $A(g.\phi) = A(\phi)$ for all $g \in G$ 
        and $\phi \in \mc{S}(\Omega,V)$, where $\mc{C}$ is a set.
    \end{itemize}

    Using these blocks we can define $G$-invariant functions $f : \mc{S}(\Omega,V) \to \mc{C}$ of the kind
    \[ f := A \circ \sigma_{i+1} \circ B_{i+1} \circ (\bigcirc_{j = 1}^i P_j \circ \sigma_j \circ B_j ) , \]
    where the input and output spaces need to be appropriately matched.

    % {An immediate observation could be that a $G$-invariant function could also only consist of one layer.
    % For instance let $f : \mc{S}(\Omega,V) \to \mc{C}$ be a linear $G$-invariant layer, because of the $G$-invariance we can write
    % $f(g.\phi) = f(\phi)$ and since $\frac{1}{\mu(G)} \int_G d\mu(g) = 1$\footnote{Here $\mu(g)$ is known as Haar measure on the group G.} we have that
    % \[ f(\phi) = f(g.\phi) = (\frac{1}{\mu(G)} \int_G d\mu(g)) f(g.\phi),  \]
    % the invariance allows us to insert $f$ inside the integral
    % \[ f(\phi) = \frac{1}{\mu(G)} \int_G d\mu(g) f(g.\phi) = f(\frac{1}{\mu(G)} \int_G d\mu(g) (g.\phi)), \]
    % where in the last equivalence we used the linearity of $f$.

    % This means that $f$ depends on $\phi$ through the $G$-average.
    % In the case of images with the translation group this would imply that $f$ only depends on the average $RGB$ value of the image.}
    Linear invariants are really limited, linear equivariants are a bigger family of operators that allows the network to reach a good approximation
    power. A brief explanation of the limitedness of linear invariants can be found in \cite{2021geo} at $3.5$.
    
    Here we are not giving a recipe for an architecture, we are rather listing the necessary conditions for the network to behave well w.r.t. the symmetries.

\end{document}