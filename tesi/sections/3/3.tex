\documentclass[../3.tex]{subfiles}
\begin{document}

    In order to construct a convolutional neural network that classifies data defined on a graph we need a definition of convolution on graphs.
    To do this we recall that in Chapter \ref{ch:2} we saw that the Fourier diagonalizes the convolution. Here we want to define a convolution on graphs
    based on this fact. First we introduce what we call \ii{graph Fourier transform}.
    
    \begin{defn}
        Let $\ket{f} \in C_0$ and $dimC_0 = n_0$, we define the \ii{graph Fourier transform}\\
        $\mc{F}_0 : \R^{n_0} \to \R^{n_0}$ to be
        \[ (\braket{i}{f})_{i \in I} \mapsto (\braket{e_i}{f})_{i \in I},\]
        where $I = \{1,...,n_0\}$ and $\ch{e_i}$ are the eigenfunctions of the $0$-laplacian.
    \end{defn}

    This transform only defines a change of basis since $\ket{f} = \sum_{i \in I}\braket{i}{f}\ket{i} = \sum_{i \in I}\braket{e_i}{f}\ket{e_i}$, and therefore is invertible.
    We can in fact represent the graph Fourier transform with the matrix $F^{-1}_{ij} = F^\dagger_{ij} := \braket{e_i}{j}$, and its inverse $F_{ij} := \braket{i}{e_j}$.
    To define a convolution between two $0$-chains we use the famous convolution theorem $\mc{F}(f * \psi) = \mc{F}(f)\mc{F}(\psi)$. 

    \begin{defn}
        Let $\{\ket{e_i}\}_{i \in I}$ be a basis such that $\Delta_p\ket{e_i} = \lambda_i\ket{e_i}$, let $\ket{f},\ket{\psi} \in C_p$, we define the representatives of $\ket{f * \psi}$ on
        the laplacian eigenchains to be 
        \[ \braket{e_i}{f * \psi} := \braket{e_i}{f}\braket{e_i}{\psi} \quad \forall i \in I.\]
        Therefore $\ket{f * \psi} = \sum_{i \in I}\braket{e_i}{f}\braket{e_i}{\psi}\ket{e_i}$.
    \end{defn}

    This can be trivially extended to simplicial complexes \footnote{In general on any chain complex} using the eigenfunctions of higher dimensional laplacians, as done in \cite{simplicialNN}.

\end{document}
