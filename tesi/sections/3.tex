\documentclass[../main.tex]{subfiles}

\begin{document}

    Learning generic functions in high dimensions is a cursed estimation
    problem, yet since most tasks of interest are not generic, and come with essential
    pre-defined regularities arising from the underlying low-dimensionality
    and structure of the physical world. 
    The aim of geometric deep learning is to exploit those regularities to reduce the dimensionality of the problem.

    Exploiting the symmetries of a system to reduce the dimensionality of a problem is a well known 
    technique in physics. It is not absurd to think that symmetries can also be used in deep learning
    to reduce the number of independent parameters to be learnt.

    The most recent formulation of geometric deep learning, which I shall follow in this chapter, is the one presented in \cite{2021geo}. 

    \begin{section}{Generalities on Convolution}
        \label{sec:3:1}
        \subfile{sections/3/1}   
    \end{section}
    \begin{section}{The Space of Signals}
        \subfile{sections/3/2}   
    \end{section}
    \begin{section}{Symmetries, Invariance and Equivariance}
        \subfile{sections/3/3}   
    \end{section}
    % \begin{section}{}
    %     \subfile{sections/3/4}   
    % \end{section}
    \begin{section}{Spectral Convolution}
        \subfile{sections/3/5}  
        \label{sec:3:3} 
    \end{section}
    % \begin{section}{Message Passing Neural Networks}
    %     \subfile{sections/3/4}  
    % \end{section}
\end{document}
